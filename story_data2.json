{"heading": "you are a programmer in python, spark, airflow, hdfs, hive. Your challenge is to implement a project from scratch where you need to ingest 12 feeds, and design a data model. There are complex dependencies between the feeds which you need to orchestrate. You then need to implement this on the cloud with an hdfs/spark platform. Show me the boiler plate code and write an engineering blog as to how you did it. Generate the blog post in simple html (just the body section of html and only use h1, h2, p and br tags).  The length should be between 3000-5000 words.",
"title": "Building out a data platform on the cloud",
  "tags": "Data Engineering, Big Data, Data computation"
}
